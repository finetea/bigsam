---
title: "보험 사기 예측"
author: "윤지환"
date: "2016년 9월 1일"
output: html_document
---

# 보험사기예측
#### 업데이트 사항

  * 골절과 관련된 증상 및 사기 연관성이 높은 특정 병명이 있는 것으로 파악 
    * M,Q,G 뼈, 선천적 기형, 뇌 및 신경계통
    * 병명 sheet가 정리되어 있는 사이트 http://wpwsyn.tistory.com/152  상병기호와 CLAIM의 RESL_CD1과 동일하게 mapping
  * *DATE변수가 영향을 미칠것으로 예상되어 YEAR 요약변수 생성 검증
  

```{r}
#Set path
setwd("C:\\Users\\rnfmf\\workspace\\bigsam\\study")

# CSV Raw Data 불러오기
RAW_BGCON_CLAIM_DATA <- read.csv("data/BGCON_CLAIM_DATA.csv")
RAW_BGCON_CNTT_DATA <- read.csv("data/BGCON_CNTT_DATA.csv")
RAW_BGCON_CUST_DATA <- read.csv("data/BGCON_CUST_DATA.csv") # 기준 데이터
RAW_BGCON_FMLY_DATA <- read.csv("data/BGCON_FMLY_DATA.csv")
RAW_BGCON_FPINFO_DATA <- read.csv("data/BGCON_FPINFO_DATA.csv")

# CSV Raw Data 불러오기
BGCON_CLAIM_DATA <- read.csv("data/BGCON_CLAIM_DATA_PROCESSED.csv")
BGCON_CNTT_DATA <- read.csv("data/BGCON_CNTT_DATA_PROCESSED.csv")
BGCON_CUST_DATA <- read.csv("data/BGCON_CUST_DATA_PROCESSED.csv") # 기준 데이터
BGCON_FMLY_DATA <- read.csv("data/BGCON_FMLY_DATA_PROCESSED.csv")
BGCON_FPINFO_DATA <- read.csv("data/BGCON_FPINFO_DATA_PROCESSED.csv")

```

## Feature Importance 확인을 위한 모델 테스트
###CART(Classification And Regression Tree)
###Logistic Regression

  * ctree(Conditional Inference Tree)
  * BGCON_CNTT_FPINFO_DATA.df.down
  * train 16386, test 4096
  * Accuracy 0.6787109 (2780/4096)

  * BGCON_CUST_FMLY_DATA.df.down
  * train 2962, test 740
  * Accuracy 0.5851351 (433/740)

###Model Test
####Logistic Regression

  * Step wise를 이용하여 종속변수집단을 선택하였더니 정확도가 유의미하게 변하였음

```
#install.packages("rattle")
#install.packages("rpart.plot")
#install.packages("caret")
suppressPackageStartupMessages({
  library(rpart)
  library(party)
  library(caret)
  library(pROC)
})
#memory.limit(100000)

#입력할 데이터
df.train <- BGCON_CUST_FMLY_DATA.df.down
df.train$Class <- NULL

set.seed(23)
training.rows <- createDataPartition(df.train$SIU_CUST_YN,  p = 0.8, list = FALSE)
train.batch <- df.train[training.rows, ]
test.batch <- df.train[-training.rows, ]

set.seed(23)
step(glm(SIU_CUST_YN ~ + DIVIDED_SET + SEX + AGE + RESI_COST + FP_CAREER + CTPR + OCCP_GRP_1 + OCCP_GRP_2 + RCBASE_HSHD_INCM + VLID_HOSP_OTDA_TOTAL + DMND_AMT_TOTAL + PAYM_AMT_TOTAL + REAL_PAYM_TERM_TOTAL, data = train.batch, family=binomial("logit")))




train.batch$CUST_RGST_YEAR <- NULL
train.batch$MAX_PAYM_YM_YEAR <- NULL
train.batch$FMLY_RELN_CODE <- NULL
train.batch$SUB_CUST_ID <- NULL
train.batch$JPBASE_HSHD_INCM <- NULL
train.batch$CUST_INCM <- NULL

train.batch$MAX_PRM <- NULL
train.batch$MAX_PAYM_YM <- NULL
train.batch$CHLD_CNT <- NULL
train.batch$MAXCRDT <- NULL
train.batch$MINCRDT <- NULL
train.batch$TOTALPREM <- NULL


train.batch$CUST_RGST <- NULL
train.batch$RESI_TYPE_CODE <- NULL
train.batch$HOUSE_HOSP_DIST_TOTAL <- NULL
train.batch$SELF_CHAM_TOTAL <- NULL

train.batch$NON_PAY_TOTAL <- NULL
train.batch$TAMT_SFCA_TOTAL <- NULL
train.batch$PATT_CHRG_TOTA_TOTAL <- NULL
train.batch$DSCT_AMT_TOTAL <- NULL
train.batch$COUNT_TRMT_ITEM_TOTAL <- NULL
train.batch$MAIN_INSR_AMT_TOTAL <- NULL
train.batch$MNTH_INCM_AMT_TOTAL <- NULL
train.batch$DISTANCE_TOTAL <- NULL
train.batch$FMLY_RELN_CODE <- NULL
train.batch$SUM_ORIG_PREM_TOTAL <- NULL
str(train.batch)
print_missing_count(train.batch)


set.seed(23)
step(glm(SIU_CUST_YN ~ ., data = train.batch, family=binomial("logit")))


glm.tune.1 <- train(SIU_CUST_YN ~ RESI_COST + CTPR + OCCP_GRP_1 + VLID_HOSP_OTDA_TOTAL + PAYM_AMT_TOTAL
                    ,data = train.batch
                    ,method = "glm"
                    ,metric = "ROC"  ## ROC 구체적으로는 AUC 가 최대가 되도록 최적화 하겠다는 의미임
                    ,trControl = cv.ctrl)
glm.tune.1
summary(glm.tune.1)

#### Model Evaluation
glm.pred.1 <- predict(glm.tune.1, test.batch)
confusion <- (confusionMatrix(glm.pred.1, test.batch$SIU_CUST_YN))

# F-Score: 2 * precision(Pos Pred Value) * recall(Sensitivity) /(precision + recall):
f1_score <- (2 * confusion$byClass[3] * confusion$byClass[1]) / (confusion$byClass[3] + confusion$byClass[1])
names(f1_score) <- "F1 Score"
f1_score

```

```
Call:
NULL

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-6.1285  -0.8155  -0.0002   0.8389   2.3852  

Coefficients:
                       Estimate Std. Error z value Pr(>|z|)    
(Intercept)          -2.834e+00  5.005e-01  -5.661 1.50e-08 ***
RESI_COST            -1.198e-05  3.577e-06  -3.350 0.000810 ***
CTPR강원              1.908e+00  7.110e-01   2.684 0.007268 ** 
CTPR경기              1.750e+00  6.704e-01   2.611 0.009041 ** 
CTPR경남              1.140e+00  6.895e-01   1.654 0.098147 .  
CTPR경북              1.684e+00  7.076e-01   2.380 0.017328 *  
CTPR광주              2.588e+00  6.883e-01   3.760 0.000170 ***
CTPR대구              1.188e+00  7.043e-01   1.687 0.091682 .  
CTPR대전              2.992e+00  7.244e-01   4.131 3.62e-05 ***
CTPR부산              2.196e+00  6.809e-01   3.225 0.001260 ** 
CTPR서울              2.134e+00  6.745e-01   3.164 0.001555 ** 
CTPR세종             -2.858e+00  5.148e+00  -0.555 0.578715    
CTPR울산              1.179e+00  7.338e-01   1.606 0.108195    
CTPR인천              2.432e+00  6.797e-01   3.578 0.000347 ***
CTPR전남              1.655e+00  6.870e-01   2.408 0.016023 *  
CTPR전북              1.848e+00  6.920e-01   2.670 0.007588 ** 
CTPR제주             -1.291e+01  3.351e+02  -0.039 0.969277    
CTPR충남              1.795e+00  7.173e-01   2.502 0.012342 *  
CTPR충북              4.119e-01  7.802e-01   0.528 0.597543    
OCCP_GRP_11.주부      1.823e-01  5.992e-01   0.304 0.760931    
OCCP_GRP_12.자영업    4.184e-01  6.043e-01   0.692 0.488759    
OCCP_GRP_13.사무직    1.854e-01  6.009e-01   0.309 0.757628    
OCCP_GRP_14.전문직   -1.832e-01  6.185e-01  -0.296 0.767107    
OCCP_GRP_15.서비스    1.114e-01  6.061e-01   0.184 0.854167    
OCCP_GRP_16.제조업   -7.257e-01  6.300e-01  -1.152 0.249371    
OCCP_GRP_17.1차산업  -9.806e-01  7.656e-01  -1.281 0.200223    
OCCP_GRP_18.기타     -2.847e-01  6.020e-01  -0.473 0.636253    
VLID_HOSP_OTDA_TOTAL  1.371e-02  7.428e-04  18.459  < 2e-16 ***
PAYM_AMT_TOTAL        1.004e-08  4.681e-09   2.146 0.031894 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 4106.2  on 2961  degrees of freedom
Residual deviance: 2937.8  on 2933  degrees of freedom
AIC: 2995.8

Number of Fisher Scoring iterations: 14
```

```
Confusion Matrix and Statistics

          Reference
Prediction   N   Y
         N 330 109
         Y  40 261
                                         
               Accuracy : 0.7986 
                 95% CI : (0.7679, 0.827)
    No Information Rate : 0.5            
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.5973         
 Mcnemar's Test P-Value : 2.536e-08      
                                         
            Sensitivity : 0.8919         
            Specificity : 0.7054         
         Pos Pred Value : 0.7517         
         Neg Pred Value : 0.8671         
             Prevalence : 0.5000         
         Detection Rate : 0.4459         
   Detection Prevalence : 0.5932         
      Balanced Accuracy : 0.7986         
                                         
       'Positive' Class : N 

```


####SVM

  * Logistic Regression 보다 성능이 떨어지는 것으로 보아 데이터의 특성이 특정 변수 필드에 걸려지는 Simple한 결과가 아닌것으로 보임
    * Tree Based Model이 적합할 것으로 보임

```

#입력할 데이터
df.train <- BGCON_CUST_FMLY_DATA.df.down
df.train$Class <- NULL

set.seed(23)
training.rows <- createDataPartition(df.train$SIU_CUST_YN,  p = 0.8, list = FALSE)
train.batch <- df.train[training.rows, ]
test.batch <- df.train[-training.rows, ]


#### SVM Model Train
set.seed(23)
svm.tune.1 <- train(SIU_CUST_YN ~ RESI_COST + CTPR + OCCP_GRP_1 + VLID_HOSP_OTDA_TOTAL + PAYM_AMT_TOTAL
                    ,data = train.batch
                    ,method = "svmRadial"
                    ,tuneLength = 9
                    ,preProcess = c("center", "scale")
                    ,metric = "ROC"
                    ,trControl = cv.ctrl)
svm.tune.1
summary(svm.tune.1)

#### SVM Model Evaluation
svm.pred.1 <- predict(svm.tune.1, test.batch)
confusion <- confusionMatrix(svm.pred.1, test.batch$SIU_CUST_YN) 


# F-Score: 2 * precision(Pos Pred Value) * recall(Sensitivity) /(precision + recall):
f1_score <- (2 * confusion$byClass[3] * confusion$byClass[1]) / (confusion$byClass[3] + confusion$byClass[1])
names(f1_score) <- "F1 Score"
f1_score

```


```
Confusion Matrix and Statistics

          Reference
Prediction   N   Y
         N 314 116
         Y  56 254
                                          
               Accuracy : 0.7676          
                 95% CI : (0.7354, 0.7976)
    No Information Rate : 0.5             
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.5351          
 Mcnemar's Test P-Value : 6.837e-06       
                                          
            Sensitivity : 0.8486          
            Specificity : 0.6865          
         Pos Pred Value : 0.7302          
         Neg Pred Value : 0.8194          
             Prevalence : 0.5000          
         Detection Rate : 0.4243          
   Detection Prevalence : 0.5811          
      Balanced Accuracy : 0.7676          
                                          
       'Positive' Class : N    
```

#### CART 

  * tree 기반 테스트
  

```
suppressPackageStartupMessages({
  library(rattle)
})

#입력할 데이터
df.train <- BGCON_CUST_FMLY_DATA.df.down
df.train$Class <- NULL

set.seed(23)
training.rows <- createDataPartition(df.train$SIU_CUST_YN,  p = 0.8, list = FALSE)
train.batch <- df.train[training.rows, ]
test.batch <- df.train[-training.rows, ]

#rpart
set.seed(23)
rpart.tune.1 <- train(SIU_CUST_YN ~ + SEX + AGE + RESI_COST + FP_CAREER + CTPR + OCCP_GRP_1 + OCCP_GRP_2 + RCBASE_HSHD_INCM + VLID_HOSP_OTDA_TOTAL + DMND_AMT_TOTAL + PAYM_AMT_TOTAL + REAL_PAYM_TERM_TOTAL
                      ,data = train.batch
                      ,method="rpart")
rpart.tune.1
summary(rpart.tune.1)
plot(rpart.tune.1)
fancyRpartPlot(rpart.tune.1$finalModel)

####Model Evaluation
rpart.pred.1 <- predict(rpart.tune.1, test.batch)
confusionMatrix(rpart.tune.1, test.batch$SIU_CUST_YN) 



#ctree를 이용해서 트리 분류
model_ctree.1 <- ctree(SIU_CUST_YN ~ + DIVIDED_SET + SEX + AGE + RESI_COST + FP_CAREER + CTPR + OCCP_GRP_1 + OCCP_GRP_2 + RCBASE_HSHD_INCM + VLID_HOSP_OTDA_TOTAL + DMND_AMT_TOTAL + PAYM_AMT_TOTAL + REAL_PAYM_TERM_TOTAL, data = train.batch)

model_ctree.1
summary(model_ctree.1)
plot(model_ctree.1)

```


#### random forest


```

#### Custom parameter setup
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes


#입력할 데이터
df.train <- BGCON_CUST_FMLY_DATA.df.down
df.train$Class <- NULL

set.seed(23)
training.rows <- createDataPartition(df.train$SIU_CUST_YN,  p = 0.8, list = FALSE)
train.batch <- df.train[training.rows, ]
test.batch <- df.train[-training.rows, ]

#mtry : 노드를 나눌 기준을 정할 때 고려할 변수
#ntree : 각 트리가 생성할 갯수
rf.grid <- expand.grid(mtry=c(16), ntree=c(7000)) 

#### RF Model Train
set.seed(23)
rf.tune.1 <- train(SIU_CUST_YN ~ RESI_COST + CTPR + OCCP_GRP_1 + VLID_HOSP_OTDA_TOTAL + PAYM_AMT_TOTAL
                    ,data = train.batch
                    ,method = customRF
                    ,metric = "ROC"
                    ,tuneGrid = rf.grid
                    ,trControl = cv.ctrl)
rf.tune.1
summary(rf.tune.1)
plot(rf.tune.1)

#### Model Evaluation
rf.pred.1 <- predict(rf.tune.1, test.batch)
confusion <- confusionMatrix(rf.pred.1, test.batch$SIU_CUST_YN)


# F-Score: 2 * precision(Pos Pred Value) * recall(Sensitivity) /(precision + recall):
f1_score <- (2 * confusion$byClass[3] * confusion$byClass[1]) / (confusion$byClass[3] + confusion$byClass[1])
names(f1_score) <- "F1 Score"
f1_score

```


#### random forest based Feature Importance Plot


```

#install.packages("rattle")
suppressPackageStartupMessages({
  library(randomForest)
})

rf.feature.importance <- randomForest(SIU_CUST_YN ~ + SEX + AGE + RESI_COST + FP_CAREER + CTPR + OCCP_GRP_1 + OCCP_GRP_2 + RCBASE_HSHD_INCM + VLID_HOSP_OTDA_TOTAL + DMND_AMT_TOTAL + PAYM_AMT_TOTAL + REAL_PAYM_TERM_TOTAL, data = train.batch, importance = TRUE)

importance(rf.feature.importance)
varImpPlot(rf.feature.importance, main = "varImpPlot")

```

#### Positive Rule
BGCON_CLAIM_DATA

  * 


#### Negative Rule

BGCON_CLAIM_DATA
  * ACCI_OCCP_GRP2
    * 고소득 전문직 
    * 법무직 종사자
  * RESL_CD1
    * F 병명 코드
    * Z 병명 코드 (기타 쪽으로 분류되는 질환)
  * CRNT_PROG_DVSN
    * 21? 
    * 22?
  * DMND_RESN_CODE
    * 9?
  * COUNT_TRMT_ITEM
    * 0
    * 4
  * VLID_HOSP_OTDA
    * 0 이상?
     
CUST
  * CHLD_CNT
    * 5?
  * MATE_OCCP_GRP_2 
    * 대학교수/강사
    * 고소득 전문직
  * MAXCRDT
    * 28?
  * OCCP_GRP_2
    * 고소득 전문직 
    * 법무직 종사자
  * CTPR
    * 제주 
    * 법무직 종사자
  * BEFO_JOB
    * 경찰
    * 국영기업체
    * 농수축산업




